{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API = '--replicate에서 발급 받은 API Token--'\n",
    "AGENT = '--브라우저 agent--'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import replicate\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl news & company information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 데이터 중 8월 1일 데이터 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 경로 설정\n",
    "csv_file_path = \"NASDAQ_RSS_IFO_202308.csv\"\n",
    "\n",
    "# \"url\" 열을 기반으로 기사 본문을 웹 크롤링하여 \"article\" 열 추가하는 함수\n",
    "def get_articles_for_rows(df, start_idx, end_idx):\n",
    "    articles = []\n",
    "    \n",
    "    for url in df.iloc[start_idx:end_idx, 7]:\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': AGENT\n",
    "            }\n",
    "\n",
    "            # Retry 설정\n",
    "            retry_strategy = Retry(\n",
    "                total=3,\n",
    "                backoff_factor=1,\n",
    "                status_forcelist=[500, 502, 503, 504],\n",
    "                method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "            )\n",
    "\n",
    "            # Retry를 적용한 세션 생성\n",
    "            session = requests.Session()\n",
    "            session.mount(\"http://\", HTTPAdapter(max_retries=retry_strategy))\n",
    "            session.mount(\"https://\", HTTPAdapter(max_retries=retry_strategy))\n",
    "\n",
    "            # 요청 사이에 1초 대기\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Retry를 적용하여 요청 보내기\n",
    "            response = session.get(url, headers=headers, timeout=30)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                html_content = response.text\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "                # 원하는 요소를 추출하여 기사 본문 가져오기 (웹 사이트에 따라 다를 수 있음)\n",
    "                article_text = \"\"\n",
    "                for p_tag in soup.find_all('p'):\n",
    "                    article_text += p_tag.get_text() + '\\n'\n",
    "                \n",
    "                articles.append(article_text)\n",
    "            else:\n",
    "                articles.append(\"\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching article: {e}\")\n",
    "            articles.append(\"\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(csv_file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# \"rgs_dt\" 열이 '20230801'인 행만 선택\n",
    "df = df[df['rgs_dt'] == 20230801]\n",
    "\n",
    "# 중복된 \"url_ifo\" 행 삭제\n",
    "df = df.drop_duplicates(subset='url_ifo', keep='first')\n",
    "\n",
    "# \"url\" 열을 기반으로 \"article\" 열 추가\n",
    "df['article'] = get_articles_for_rows(df, 0, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.copy()\n",
    "\n",
    "tickers = []\n",
    "tickers.extend(temp['tck_iem_cd'].values.flatten().tolist())\n",
    "\n",
    "rld_tickers = temp['rld_ose_iem_tck_cd'].apply(lambda x:x.split(',') if x!='_' else [])\n",
    "\n",
    "for i in rld_tickers.values:\n",
    "    tickers.extend(i)\n",
    "    \n",
    "tickers = [tic.strip().lower() for tic in tickers]\n",
    "\n",
    "tickers = list(set(tickers))\n",
    "\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent':AGENT}\n",
    "\n",
    "company_info = {}\n",
    "company_industry = {}\n",
    "\n",
    "parse_err = []\n",
    "info_err = []\n",
    "indust_err = []\n",
    "unavailable_ticker = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tqdm(tickers):\n",
    "    url = f'https://stockanalysis.com/stocks/{ticker}/company/'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url,headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    except:\n",
    "        parse_err.append(ticker)\n",
    "        continue\n",
    "    try:\n",
    "        check = soup.select(\"main > div > div.mb-4.text-2xl.font-bold.sm\\:text-3xl\")[0].text\n",
    "        if '404' in check:\n",
    "            unavailable_ticker.append(ticker)\n",
    "            continue\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        desc = soup.select(\"main > div:nth-child(3) > div:nth-child(1)\")\n",
    "        if len(desc)==0:\n",
    "            desc = soup.select(\"main > div.mt-4.sm\\:mt-5 > div:nth-child(1)\")\n",
    "        company_info[ticker] = desc[0].text\n",
    "    except:\n",
    "        info_err.append(ticker)\n",
    "        \n",
    "    try:\n",
    "        num = 0\n",
    "        selector = 1\n",
    "        for i in range(1,7):\n",
    "            industry = soup.select(f'main > div:nth-child(3) > div.lg\\:float-right.lg\\:w-\\[336px\\] > div.mt-7.rounded.border.border-gray-200.bg-gray-50.px-3.pt-3.pb-2.dark\\:border-dark-700.dark\\:bg-dark-775.xs\\:px-4.xs\\:pt-4.lg\\:mt-1 > table > tbody > tr:nth-child({i}) > td.py-1\\.5.px-1.font-semibold.lg\\:py-2')\n",
    "            if len(industry)==0:\n",
    "                selector = 2\n",
    "                industry = soup.select(f'main > div.mt-4.sm\\:mt-5 > div.lg\\:float-right.lg\\:w-\\[336px\\] > div.mt-7.rounded.border.border-gray-200.bg-gray-50.px-3.pt-3.pb-2.dark\\:border-dark-700.dark\\:bg-dark-775.xs\\:px-4.xs\\:pt-4.lg\\:mt-1 > table > tbody > tr:nth-child({i}) > td.py-1\\.5.px-1.font-semibold.lg\\:py-2')\n",
    "            if industry[0].text=='Industry':\n",
    "                num = i\n",
    "                break\n",
    "        if num==0:\n",
    "            indust_err.append(ticker)\n",
    "        else:\n",
    "            if selector == 1:\n",
    "                text = soup.select(f\"main > div:nth-child(3) > div.lg\\:float-right.lg\\:w-\\[336px\\] > div.mt-7.rounded.border.border-gray-200.bg-gray-50.px-3.pt-3.pb-2.dark\\:border-dark-700.dark\\:bg-dark-775.xs\\:px-4.xs\\:pt-4.lg\\:mt-1 > table > tbody > tr:nth-child({num}) > td.py-1\\.5.px-1.text-right.lg\\:py-2 > a\")\n",
    "            else:\n",
    "                text = soup.select(f\"main > div.mt-4.sm\\:mt-5 > div.lg\\:float-right.lg\\:w-\\[336px\\] > div.mt-7.rounded.border.border-gray-200.bg-gray-50.px-3.pt-3.pb-2.dark\\:border-dark-700.dark\\:bg-dark-775.xs\\:px-4.xs\\:pt-4.lg\\:mt-1 > table > tbody > tr:nth-child({num}) > td.py-1\\.5.px-1.text-right.lg\\:py-2 > a\")\n",
    "                \n",
    "            company_industry[ticker] = text[0].text\n",
    "    except:\n",
    "        indust_err.append(ticker)\n",
    "        \n",
    "    time.sleep(2)\n",
    "    if len(info_err)>1:\n",
    "        print('error')\n",
    "        break\n",
    "        \n",
    "info_err = list(set(info_err)-set(list(company_info.keys()))-set(unavailable_ticker))\n",
    "indust_err = list(set(indust_err)-set(list(company_industry.keys()))-set(unavailable_ticker))\n",
    "unavailable_ticker = list(set(unavailable_ticker)-set(list(company_info.keys()))-set(list(company_industry.keys())))\n",
    "parse_err = list(set(parse_err)-set(unavailable_ticker)-set(list(company_info.keys()))-set(list(company_industry.keys())))\n",
    "\n",
    "print('파씽 실패 :',len(parse_err))\n",
    "print('존재하지 않는 티커코드 :',len(unavailable_ticker))\n",
    "print('기업 설명 실패 :',len(info_err))\n",
    "print('사업분야 실패 :',len(indust_err))\n",
    "\n",
    "print('기업 설명 :',len(company_info))\n",
    "print('기업 사업분야 :',len(company_industry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = 'main > div.mt-6.lg\\:grid.lg\\:grid-cols-sidebar_wide.lg\\:gap-x-10 > div.space-y-6.lg\\:order-2.lg\\:pt-1 > div.px-0\\.5.lg\\:px-0 > p'\n",
    "holdings = 'main > div.mt-6.lg\\:grid.lg\\:grid-cols-sidebar_wide.lg\\:gap-x-10 > div.space-y-6.lg\\:order-2.lg\\:pt-1 > div:nth-child(4) > table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_info = {}\n",
    "etf_holdings = {}\n",
    "\n",
    "for ticker in tqdm(unavailable_ticker[136:]):\n",
    "    url = f'https://stockanalysis.com/etf/{ticker}'\n",
    "\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        check = soup.select(\"main > div > div.mb-4.text-2xl.font-bold.sm\\:text-3xl\")[0].text\n",
    "        if '404' in check:\n",
    "            #unavailable_ticker.append(ticker)\n",
    "            print(ticker,'pass')\n",
    "            continue\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    about = soup.select(desc)\n",
    "    table = soup.find_all('table')\n",
    "    print(ticker,len(about),len(table))\n",
    "    dfs = pd.read_html(str(table))\n",
    "    \n",
    "    holding_list = None\n",
    "    for df in dfs:\n",
    "        if 'Symbol'in df.columns:\n",
    "            holding_list = df\n",
    "            break\n",
    "    \n",
    "    etf_info[ticker] = about[0].text\n",
    "    etf_holdings[ticker] = holding_list\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unavailable_ticker = list(set(unavailable_ticker)-set(list(etf_info.keys()))-set(list(etf_holdings.keys())))\n",
    "len(unavailable_ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만든 dictionary를 pickle로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./company_describe.pkl','wb') as f:\n",
    "#     pickle.dump(company_info,f)\n",
    "\n",
    "# with open('./company_industry.pkl','wb') as f:\n",
    "#     pickle.dump(company_industry,f)\n",
    "\n",
    "# with open('./etf_describe.pkl','wb') as f:\n",
    "#     pickle.dump(etf_info,f)\n",
    "\n",
    "# with open('./etf_holdings.pkl','wb') as f:\n",
    "#     pickle.dump(etf_holdings,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2df(dic):\n",
    "    df = pd.DataFrame(dic, index=[0]).T\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = ['company', 'description']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_info.update(etf_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com = dict2df(company_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mitral 7b를 이용한 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replicate에서 API 발급 필요 - https://replicate.com/ <br>\n",
    "유료로 전환 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"REPLICATE_API_TOKEN\"] = API\n",
    "df_com['summary_en'] = None\n",
    "df['summary_en'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mistral(article):\n",
    "    output = replicate.run(\n",
    "    \"a16z-infra/mistral-7b-v0.1:3e8a0fb6d7812ce30701ba597e5080689bef8a013e5c6a724fafb108cc2426a0\",\n",
    "    input={\"prompt\": f\"You are analyst. Can you summarize following breifly?\\n{article[:3500]}\"}\n",
    "    )\n",
    "    result = \"\"\n",
    "    for item in output:\n",
    "        result += item\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 요약\n",
    "for i in tqdm(df.index):\n",
    "    df['summary_en'].loc[i] = summarize_mistral(df['article'].loc[i])\n",
    "\n",
    "# 기업 설명 요약\n",
    "for i in tqdm(df_com.index):\n",
    "    df_com['summary_en'].loc[i] = summarize_mistral(df_com['description'].loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
